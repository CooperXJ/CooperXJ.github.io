<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cooperxj.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="记录一些学习笔记、小说、随笔">
<meta property="og:type" content="website">
<meta property="og:title" content="Cooper Notes">
<meta property="og:url" content="http://cooperxj.github.io/page/15/index.html">
<meta property="og:site_name" content="Cooper Notes">
<meta property="og:description" content="记录一些学习笔记、小说、随笔">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Cooper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://cooperxj.github.io/page/15/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Cooper Notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Cooper Notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">obey rules</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/34-35join%E6%9F%A5%E8%AF%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/34-35join%E6%9F%A5%E8%AF%A2/" class="post-title-link" itemprop="url">Join查询</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：15:00:58" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/34-35join%E6%9F%A5%E8%AF%A2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/34-35join%E6%9F%A5%E8%AF%A2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Join查询"><a href="#Join查询" class="headerlink" title="Join查询"></a>Join查询</h3><p><code>select * from t1 straight_join t2 on (t1.a=t2.a)</code> </p>
<p>t1和t2的a字段上有索引</p>
<ul>
<li>index nested-loop join （建议）</li>
</ul>
<p>​    如果被驱动表查询的字段有索引，在join查询相比较于单表查询（先查询t1的所有数据再带入到t2中进行查询）是有优势的</p>
<p>​    该种join查询的扫描行数为N+N<em>2</em>log<sub>2</sub>M  (N为t1的行数，M为t2的行数) （<em>2</em>log<sub>2</sub>M是因为依靠数查询需要两次，一次是走索引a，一次是走主键索引）</p>
<ul>
<li><p>如果被驱动表上没有索引</p>
<ul>
<li><p>simple nested-loop join （非常不建议，MySQL并没有使用该索引）</p>
<p>被驱动表查询的字段没有索引</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>block nested-loop join（MySQL选择的方式）</p>
<p>算法执行流程如下：</p>
<ol>
<li>将表a的需要查询的字段都放入buffer中</li>
<li>对于被驱动表顺序扫描每一行数据，判断join条件是否满足，如果语句中有where语句则先使用where语句进行过滤</li>
<li>将满足条件的数据进行返回</li>
</ol>
<p>有时候由于表过大，因此需要分段进行比较大小，由于是存放在内存中的，因此虽然时间复杂度上和<code>simple nested-loop join</code>差不多，但是是在内存中比较，因此速度快一些</p>
<p>如果使用了该种排序，explain会出现<code>block nested-loop join</code>这个字段</p>
<ul>
<li><p>需要进行分段比较，扫描行数为N+$\lambda$$\times$N$\times$M（$\lambda$是分段的数量，和join_buffer_size、N有关）</p>
<p>因此join_buffer_size越大，扫描行数越少</p>
</li>
</ul>
</li>
</ul>
<p>​      <font color=red>缺陷：</font></p>
<ol>
<li><p>对于IO有较大影响</p>
</li>
<li><p>需要多次判断，如果是大表则会占用较多的CPU</p>
</li>
<li><p>可能会导致buffer pool的热数据淘汰，影响内存命中率</p>
<p>  情况如下：</p>
<ul>
<li>多次扫描一个冷表，并且语句执行时间超过1s，则下次扫描该冷表时就会将冷表的数据页移到链表头部</li>
<li>一个正常的数据页想要进行young区域，必须隔1s，但是由于join的不断读磁盘和淘汰内存页，有可能进入old区域的数据页在1s内就被淘汰了</li>
</ul>
</li>
</ol>
<h3 id="Join查询原则"><a href="#Join查询原则" class="headerlink" title="Join查询原则"></a>Join查询原则</h3><p>小表驱动大表</p>
<h4 id="小表的定义"><a href="#小表的定义" class="headerlink" title="小表的定义"></a>小表的定义</h4><p>两个表按照各自的条件进行过滤完成之后，计算参与join各个字段的总数据量，数据量小的那个表就是小表</p>
<h3 id="Join优化"><a href="#Join优化" class="headerlink" title="Join优化"></a>Join优化</h3><h4 id="前置知识-Multi-Range-Read优化"><a href="#前置知识-Multi-Range-Read优化" class="headerlink" title="前置知识  Multi-Range Read优化"></a>前置知识  Multi-Range Read优化</h4><ul>
<li><p>优化目的</p>
<p>使得随机读变为顺序读</p>
</li>
<li><p>优化流程</p>
<ol>
<li><p>根据索引将满足条件的记录放到read_rnd_buffer中（read_rnd_buffer的大小由read_rnd_buffer确定，默认为256k）</p>
</li>
<li><p>将read_rnd_buffer中的id进行递增排序</p>
</li>
<li><p>根据排序后的id数组，依次到主键id索引中查询记录并返回</p>
<p>如果步骤1中read_rnd_buffer放满了，则先执行2，3，然后循环</p>
</li>
</ol>
</li>
<li><p>启用MRR</p>
<p>现在的优化器不倾向于使用MRR，如果想要稳定使用MRR，则需要设置<code>set optimizer_switch=&quot;mrr_cost_based=off&quot;</code></p>
</li>
<li><p>体现</p>
<p>explain了之后会显示using MRR</p>
</li>
</ul>
<h4 id="使用index-nested-loop-join时用到的优化-（Batched-Key-Access-）"><a href="#使用index-nested-loop-join时用到的优化-（Batched-Key-Access-）" class="headerlink" title="使用index nested-loop join时用到的优化 （Batched Key Access ）"></a>使用index nested-loop join时用到的优化 （Batched Key Access ）</h4><ul>
<li><p>核心思想</p>
<p>将驱动表查询到的id放入到join_buffer中，然后将整个join_buffer中的id与被驱动表中的数据进行对比</p>
</li>
<li><p>好处</p>
<p>避免了查询驱动表中的一条数据就和被驱动表中的数据进行比较</p>
</li>
<li><p>启用BKA</p>
<p><code>set optimizer_switch=&quot;mrr=on,mrr_cost_based=off,batched_key_access=on&quot;</code></p>
</li>
</ul>
<h4 id="使用block-nested-loop-join时用到的优化-（Batched-Key-Access-）"><a href="#使用block-nested-loop-join时用到的优化-（Batched-Key-Access-）" class="headerlink" title="使用block nested-loop join时用到的优化 （Batched Key Access ）"></a>使用block nested-loop join时用到的优化 （Batched Key Access ）</h4><ul>
<li><p>核心思想</p>
<p>在被驱动表上加上索引，将BNL转为NLJ，再利用Batched Key Access</p>
</li>
<li><p>问题</p>
<p>当被驱动表需要查询到的数据比较少，但被驱动表又比较大时，建立索引是不合适的</p>
<ul>
<li>解决法案<ul>
<li>建立临时索引表</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="hash-join"><a href="#hash-join" class="headerlink" title="hash join"></a>hash join</h4><ul>
<li><p>核心思想</p>
<p>如果join_buffer中维护的不是一个无序数组，而是一个哈希表，则只需要每次先查询被驱动表的数据，然后查看该记录是否在join_buffer中的hash表中即可</p>
</li>
<li><p>问题</p>
<p>官方没有安排</p>
<ul>
<li><p>解决方案</p>
<p>我们可以自己分两次查询，第一次查询驱动表然后将数据存入到hash表中，第二次查询被驱动表，然后遍历分别在hash表查询是否存在，存在则做拼接操作</p>
</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/09-%E7%B4%A2%E5%BC%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/09-%E7%B4%A2%E5%BC%95/" class="post-title-link" itemprop="url">索引</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:37:21" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/09-%E7%B4%A2%E5%BC%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/09-%E7%B4%A2%E5%BC%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>541</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="普通索引和唯一索引"><a href="#普通索引和唯一索引" class="headerlink" title="普通索引和唯一索引"></a>普通索引和唯一索引</h3><ol>
<li><p>InnoDB的数据是<strong>按照数据页为单位进行读写</strong>的，也就是当读一条记录的时候并不会将这个记录本身从磁盘中读取出来，而是以页尾单位将其整体读入内存</p>
</li>
<li><p>change buffer</p>
<p>change buffer使用必须要同时满足一下条件</p>
<ul>
<li>索引必须是辅助索引</li>
<li>索引不是唯一的</li>
</ul>
<p>change buffer的机制</p>
<p>在不影响数据一致性的前提下，InnoDB会将更新操作缓存在change buffer中，这样就不需要从磁盘中读入对应的数据页，在下次访问到这个数据页的时候将该数据叶读入内存，然后执行change buffer中与这个数据页的有关操作。</p>
<p><font color=red>注意: 如果将更新操作缓存到change buffer之后又立即对该数据页进行了访问，那么还是会触发随机读IO，将对应数据页加载到了缓存中，那么change buffer的作用也就很小</font></p>
<ul>
<li><p>change buffer不适用于唯一索引的原因</p>
<p>因为每次插入记录的时候需要检查该条记录的唯一性，会触发读取数据页，那么也就无法达到减少随机读IO的效果，也就导致change buffer失效</p>
</li>
<li><p>change buffer的使用场景</p>
<ul>
<li><p>写多读少的业务，页面在写完以后马上被访问的概率比较小，比如说账单类、日志类系统</p>
</li>
<li><p>如果所有的更新后面都马上伴随着对该记录的查询，那么应该关闭change buffer</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/19-%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/19-%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">查询速度优化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:43:02" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/19-%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/19-%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>450</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="查询语句很慢"><a href="#查询语句很慢" class="headerlink" title="查询语句很慢"></a>查询语句很慢</h3><h4 id="查询长时间不返回"><a href="#查询长时间不返回" class="headerlink" title="查询长时间不返回"></a>查询长时间不返回</h4><ol>
<li><p>等MDL写锁</p>
</li>
<li><p>等flush</p>
</li>
<li><p>等行锁（已经有了写锁）</p>
<ul>
<li><p>查询哪条语句在等待</p>
<p><code>mysql&gt; select from t sys.innodb_lock_waits where locked_table= &#39;test&#39;.&#39;t&#39;\G</code></p>
</li>
<li><p>查询到之后不应该使用<code>kill query &#123;pid&#125;</code>,因为这样只是终止当前被阻塞的查询，而占用锁的那个操作还是没有被终止，因此需要使用<code>kill &#123;pid&#125;</code>,这样等待的操作和占用当前行锁的操作都会被终止</p>
</li>
</ul>
</li>
</ol>
<h3 id="查询慢"><a href="#查询慢" class="headerlink" title="查询慢"></a>查询慢</h3><p>可能是由于mvcc的机制导致的</p>
<p>想象这样一种场景：</p>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20211031113622.png" alt="image-20211031113607948" style="zoom:50%;" />

<p>可以看到sessionA的<code>select * from t where id=1</code>查询的是100万之前的版本，而<code>select * from t where id=1 lock in share mode</code>查询的是当前版本，因此第一个查询语句需要经过100万个版本才能查询到当前sessionA的版本，而第二个查询语句只需要查询当前的版本，因此第二个查询语句比第一个查询语句快得多。</p>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/25-26MySQL%E4%B8%BB%E5%A4%87%E6%97%B6%E5%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/25-26MySQL%E4%B8%BB%E5%A4%87%E6%97%B6%E5%BB%B6/" class="post-title-link" itemprop="url">MySQL主备时延</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:58:21" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/25-26MySQL%E4%B8%BB%E5%A4%87%E6%97%B6%E5%BB%B6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/25-26MySQL%E4%B8%BB%E5%A4%87%E6%97%B6%E5%BB%B6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="MySQL主备时延"><a href="#MySQL主备时延" class="headerlink" title="MySQL主备时延"></a>MySQL主备时延</h3><h4 id="主备时延的大小"><a href="#主备时延的大小" class="headerlink" title="主备时延的大小"></a>主备时延的大小</h4><ul>
<li><p>计算方法</p>
<p>备库取出当前正在执行的事务A的时间-主库当前事务Abinlog中记录时间</p>
</li>
<li><p>相关字段</p>
<p><code>show slave status</code>中的<code>seconds_behind_master</code></p>
<p>注意：如果当前主备的系统时间不一致，备库连接到主库的时候会通过执行select unix_timestamp()函数来获取当前主库的系统时间，如果发现与自己不一致会在计算<code>seconds_behind_master</code>自动减去该差值</p>
</li>
</ul>
<h4 id="主备时延的来源"><a href="#主备时延的来源" class="headerlink" title="主备时延的来源"></a>主备时延的来源</h4><ul>
<li><p>备库所在的机器性能要比主库所在的机器性能差</p>
</li>
<li><p>备库的压力较大</p>
<p>解决方法</p>
<ul>
<li>一主多从，让从库减轻备库读的压力</li>
<li>通过binlog输出到外部系统，比如hadoop，让外部系统通过统计类查询的能力</li>
</ul>
</li>
<li><p>大事务</p>
<p>主库必须等待事务执行完成才会写入binlog，再传给备库，可以看到主库的执行时间越长，那么主备延时也就越厉害</p>
<p>常见的场景如下：</p>
<ul>
<li>一次性delete太多数据</li>
<li>大表DDL</li>
</ul>
</li>
<li><p>备库的并行复制能力</p>
</li>
</ul>
<h4 id="主备切换策略"><a href="#主备切换策略" class="headerlink" title="主备切换策略"></a>主备切换策略</h4><ul>
<li><p>可靠性优先策略</p>
<p>步骤：</p>
<ol>
<li>判断备库B现在的<code>seconds_behind_master</code>,如果小于某个值（比如5秒）继续下一步，<br>否则持续重试这一步；</li>
<li>把主库A改成只读状态，即把readonly设置为true,</li>
<li>判断备库B的<code>seconds_behind_master</code>的值，直到这个值变成0为止；</li>
<li>把备库B改成可读写状态，也就是把readonly设置为false;</li>
<li>把业务请求切到备库B</li>
</ol>
</li>
</ul>
<ul>
<li><p>可用性优先策略</p>
<p>强行将可靠性优先策略的步骤4、5调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库，让备库可以读写</p>
<p>风险：可能会引起主备不一致</p>
</li>
</ul>
<h3 id="MySQL备库并行复制"><a href="#MySQL备库并行复制" class="headerlink" title="MySQL备库并行复制"></a>MySQL备库并行复制</h3><h4 id="MySQL-5-6并行复制策略"><a href="#MySQL-5-6并行复制策略" class="headerlink" title="MySQL 5.6并行复制策略"></a>MySQL 5.6并行复制策略</h4><ul>
<li><p>mysql官方</p>
<p>支持按库并行，如果主库上存在多个DB，那么不同的DB上面SQL可以分发到不同的线程上执行</p>
</li>
<li><p>MariaDB</p>
<ol>
<li><p>在一组里面一起提交的事务（在一组里面提交的事务一定不会修改同一行），有一个相同的commitid，下一组就是commitid+1;</p>
</li>
<li><p>commitid直接写到binlog里面；</p>
</li>
<li><p>传到备库应用的时候，相同commitid的事务分发到多个worker执行；</p>
</li>
<li><p>这一组全部执行完成后，coordinator再去取下一批。</p>
</li>
</ol>
<p>缺点：如果某一组事务中有一个事务非常大，那么等待的时间会非常长，浪费资源</p>
</li>
</ul>
<h4 id="MySQL-5-7并行复制策略"><a href="#MySQL-5-7并行复制策略" class="headerlink" title="MySQL 5.7并行复制策略"></a>MySQL 5.7并行复制策略</h4><ol>
<li>同时处于prepare状态的事务，在备库执行时是可以并行的；</li>
<li>处于prepare状态的事务，与处于commit状态的事务之间，在备库执行时也是可以并行的。</li>
</ol>
<p>对于<code>binlog_group_commit_sync_delay</code>和<code>binlog_group_sync_no_delay_count</code>参数的调整可以拉长binlog从write到fsync的时间以减少binlog的写盘次数同时也可以让同组中的事务更多一些，这样在备库执行的时候并发量就会更大一些</p>
<p><font color=red>注意：”处于prepare状态的事务，可以并行“ 在实现上是，主库在写binlog的时候会给这些binlog里面记commitid和sequenceno,来说明事务之间在主库上并行prepare的状态,也就是在备库执行的时候可以并发执行</font></p>
<h4 id="MySQL-5-7-22并行策略"><a href="#MySQL-5-7-22并行策略" class="headerlink" title="MySQL 5.7.22并行策略"></a>MySQL 5.7.22并行策略</h4><p>增加了一个新的并行复制策略：<code>binlog-transaction-dependency-tracking</code></p>
<p>有三种可选值</p>
<ol>
<li><p>commit_order</p>
<p>根据同时进入的prepare和commit来判断是否可以并行</p>
</li>
<li><p>writeset</p>
<p>对于事务更新的每一行，计算出这一行的hash值组成集合writeset，如果两个事务没有操作相同的行，那么可以并行</p>
</li>
<li><p>writeset_session</p>
<p>在2的基础上多了约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候需要保证相同的先后顺序</p>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/18-SQL%E8%B0%83%E4%BC%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/18-SQL%E8%B0%83%E4%BC%98/" class="post-title-link" itemprop="url">SQL调优</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:41:48" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/18-SQL%E8%B0%83%E4%BC%98/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/18-SQL%E8%B0%83%E4%BC%98/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>339</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="对索引字段做函数操作"><a href="#对索引字段做函数操作" class="headerlink" title="对索引字段做函数操作"></a>对索引字段做函数操作</h3><ol>
<li><p>对索引字段做函数操作。可能会破坏索引值的有序性，因此优化器就会放弃走树搜索的功能</p>
<p>常见的场景如下：</p>
<ul>
<li><p>隐式类型转换</p>
<p>比如说某张表的字段是int类型，但是查询时是varchar类型，那么会mysql会将varchar类型进行转换，从而导致索引失效</p>
</li>
<li><p>隐式字符编码转换</p>
<p>比如说两张表，连接的字段的编码类型不同，一个是utf8，一个是uft8mb4，那么将这两个字段进行连接的时候会产生编码转换，也会导致索引失效</p>
</li>
<li><p>最基本的where id+1=100也会导致索引失效，因为id+1 mysql会认为破坏了索引值的有序性，需要写成where id=99才行</p>
</li>
</ul>
</li>
<li><p><font color=red>也就是我们需要保证where中“=”或者“&lt;“或者其他符合的左边列没有任何操作，一旦有任何操作将会导致优化器放弃走搜索树的功能</font></p>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/28-MySQL%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/28-MySQL%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/" class="post-title-link" itemprop="url">MySQL读写分离</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:59:24" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/28-MySQL%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/MySQL/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/28-MySQL%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="MySQL读写分离造成的过期读"><a href="#MySQL读写分离造成的过期读" class="headerlink" title="MySQL读写分离造成的过期读"></a>MySQL读写分离造成的过期读</h3><p>解决方案</p>
<ol>
<li><p>强制走主库</p>
<p>将查询请求进行分类</p>
<ul>
<li>对于必须要拿到最新结果的请求，强制将其发布到主库上</li>
<li>对于可以读到旧数据的请求，将其发到从库上</li>
</ul>
</li>
<li><p>sleep方案</p>
<p>主库更新后，读从库之前先sleep一下</p>
<p>缺点：会降低查询速度</p>
</li>
<li><p>判断主备无延迟</p>
<ul>
<li><p>判断<code>seconds_behind_master</code>是否已经等于0，等于0才能对执行查询请求</p>
</li>
<li><p>对比位点</p>
<p><code>master_log_file</code>与<code>relay_master_log_file</code>是否相等</p>
<p><code>read_master_log_pos</code>与<code>exec_master_log_pos</code>是否相等</p>
<p>如果都相等，则说明同步已完成</p>
</li>
<li><p>对比GTID</p>
<p><code>retrieved_gtid_set</code>与<code>executed_gtid_set</code>两个集合是否相等</p>
<p>相等则说明同步完成</p>
</li>
</ul>
</li>
<li><p>配合semi-sync（半同步复制）</p>
<p>出现的原因：因为有时候客户端收到来自主库的执行成功相应之后，但此时从库还没有收到来自主库的log，这时候如果使用3中的方法都会出现过期读的现象</p>
<p>semi-sync的思想</p>
<ol>
<li>事务提交时主库把binlog发给从库</li>
<li>从库收到之后返回给主库一个ack</li>
<li>主库收到ack后才能向客户端发送“事务完成”的确认</li>
</ol>
</li>
</ol>
<p>​        缺陷：对于一主多从的场景下，只要有一个从发送了ack，那么就会主就会向客户端发送确认消息，但是如果此时的查询未落在发送ack的从上时，就会发生过                    期读现象</p>
<ol start="5">
<li><p>不需要等待主备完全同步</p>
<p>思想：如果多个事务都在进行执行，然而我要查询的只是第一个事务，那么我只需要等待第一个事务同步完毕就可以进行查询了，而无需等待所有的事务都完成再进行查询</p>
<ul>
<li><p>等主库位点方案</p>
<p><code>select master_pos_wait(file, pos[, timeout])</code></p>
<p>该方法查询的是：这个命令在<font color=red>从库</font>上执行开始直到应用完file和pos表示的binlog位置执行了多少个事务</p>
<p>此处的<code>file</code>和<code>pos</code>是在主库上查询出来的，也就是当主库上执行完第一个事务之后查询出来的内容</p>
<p>使用该方法可以查询到从库从执行该语句开始直到应用到file和pos表示的binlog的位置（也就是主库上第一个事务执行完后binlog的fileName和pos）执行了多少个事务</p>
<p>查询的结果如下：</p>
<ul>
<li>如果执行期间备库同步线程发生了异常，则返回null</li>
<li>如果等待超过N秒，即超时返回-1</li>
<li>如果刚开始执行的时候就发现已经执行过这个位置，作为返回0</li>
<li>正常情况下返回这一时间段内执行事务的数量</li>
</ul>
<p>当查询的结果&gt;=0时，表示该从库一定已经有了第一个事务的数据了</p>
<p>缺陷：该方法不仅需要查询从库，同时需要查询主库</p>
</li>
<li><p>GTID方案</p>
<p><code>select wait_for_executed_gtid_set(gtid_set, 1);</code></p>
<p>该查询的意思：等待直到该库执行的事务中包含传入的gtid_set，返回0，如果超时返回1</p>
<p>也就是说只要在超时界限内该gtid_set表示的事务执行了，那么就会返回0</p>
<p>缺陷：该方法需要对mysql源码进行改造</p>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1/SpringCloud%20Gateway/API%E7%BD%91%E5%85%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1/SpringCloud%20Gateway/API%E7%BD%91%E5%85%B3/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-07 08:43:38" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-24 15:12:02" itemprop="dateModified" datetime="2021-07-24T15:12:02+08:00">2021-07-24</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1/SpringCloud%20Gateway/API%E7%BD%91%E5%85%B3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1/SpringCloud%20Gateway/API%E7%BD%91%E5%85%B3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>139</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="API网关"><a href="#API网关" class="headerlink" title="API网关"></a>API网关</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li>微服务架构统一的入口</li>
<li>是一个服务器，是系统对外的唯一入口</li>
</ul>
<h4 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h4><ul>
<li>易于监控，可以在微服务网关收集监控数据并将其推送到外部系统进行分析</li>
<li>易于认证，可以在微服务网关上进行认证，然后再将请求转发到后端的微服务，从而无需在每个微服务中进行认证</li>
<li>减少客户端宇各个微服务之间的交互次数</li>
</ul>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210724150756.png" alt="image-20210724150750864" style="zoom:50%;" />


      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/Graph%20embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/Graph%20embedding/" class="post-title-link" itemprop="url">graph embedding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:28:43" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">图网络</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/Graph%20embedding/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/Graph%20embedding/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>515</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="主要作用"><a href="#主要作用" class="headerlink" title="主要作用"></a>主要作用</h4><p>常见的表示节点的方法就是利用one-hot编码，那么有多少个节点就有多少维的向量</p>
<p>缺陷也是显而易见的：</p>
<ul>
<li>节点数量比较多的话会造成向量的长度比较大</li>
<li>丢失了节点在图上的信息</li>
</ul>
<p>graph embedding的好处</p>
<ul>
<li>简化了节点的特征长度</li>
<li>保留了节点在图上的信息</li>
</ul>
<h4 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h4><ul>
<li><p>主要思想</p>
<p>通过随机游走产生图的序列，就像wordvec2中的语料一样</p>
</li>
<li><p>使用场景</p>
<p>无向图</p>
</li>
</ul>
<h4 id="LINE"><a href="#LINE" class="headerlink" title="LINE"></a>LINE</h4><ul>
<li><p>使用场景</p>
<p>大规模图上，表示节点之间的结构信息，有向图无向图都可以、</p>
</li>
<li><p>主要思想</p>
<ul>
<li><p>一阶相似性</p>
<p>局部的结构信息 （如果两个节点之间连接并且连接的权重比较大，那么这两个节点的embedding是相似的）</p>
</li>
<li><p>二阶相似性</p>
<p>节点的邻居，共享邻居的节点可能是相似的 （如果两个节点的邻居是相似的，那么即使这两个节点之间没有连接，那么这两个节点的embedding也是相似的）</p>
</li>
<li><p>将一阶和二阶拼接在一起</p>
</li>
</ul>
</li>
</ul>
<h4 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h4><ul>
<li><p>主要思想</p>
<p>不同的随机游走策略，形成序列，类似skip-gram方式生成节点embedding</p>
</li>
</ul>
<h4 id="Struct2Vec"><a href="#Struct2Vec" class="headerlink" title="Struct2Vec"></a>Struct2Vec</h4><ul>
<li><p>使用场景</p>
<p>寻找两个不同图中相似的节点</p>
<p>适用于节点分类中，其结构标识比邻居标识更重要，采用Struct2Vec效果好</p>
</li>
<li><p>主要思想</p>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210720154602.png" alt="image-20210720154555078" style="zoom:50%;" /></li>
</ul>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210720155239.png" alt="image-20210720155227466" style="zoom:50%;" />

<p>​                                                                （动态规划）</p>
<h4 id="SDNE"><a href="#SDNE" class="headerlink" title="SDNE"></a>SDNE</h4><ul>
<li><p>主要思想</p>
<p>采用了多个非线性层的方式捕获一阶二阶的相似性</p>
</li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/%E7%A9%BA%E5%9F%9FGNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/%E7%A9%BA%E5%9F%9FGNN/" class="post-title-link" itemprop="url">空域GNN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 08:43:38 / 修改时间：14:29:06" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">图网络</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/%E7%A9%BA%E5%9F%9FGNN/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E7%BD%91%E7%BB%9C/%E7%A9%BA%E5%9F%9FGNN/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>24</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="NN4G"><a href="#NN4G" class="headerlink" title="NN4G"></a>NN4G</h4><ul>
<li><p>每一层的训练</p>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210719154532.png" alt="image-20210719154522060" style="zoom:50%;" /></li>
<li><p>输出</p>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210719154810.png" alt="image-20210719154806203" style="zoom:50%;" /></li>
</ul>
<h4 id="DCNN"><a href="#DCNN" class="headerlink" title="DCNN"></a>DCNN</h4><ul>
<li><p>每一层的训练</p>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210719155148.png" alt="image-20210719155143110" style="zoom:50%;" /></li>
<li><p>输出</p>
<img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210719155226.png" alt="image-20210719155221204" style="zoom:50%;" /></li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://cooperxj.github.io/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch/pytorch-base/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="记录一些学习笔记、小说、随笔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cooper Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch/pytorch-base/" class="post-title-link" itemprop="url">pytorch基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-07 08:43:38" itemprop="dateCreated datePublished" datetime="2022-01-07T08:43:38+08:00">2022-01-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-27 22:31:09" itemprop="dateModified" datetime="2021-08-27T22:31:09+08:00">2021-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch/pytorch-base/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch/pytorch-base/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="打开jupyter-notebook"><a href="#打开jupyter-notebook" class="headerlink" title="打开jupyter notebook"></a>打开jupyter notebook</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate pytorch</span><br><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>



<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>Pytorch里面的数据集是需要我们自己去定义的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyData</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,root_dir,label_dir</span>):</span></span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir,self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果获取每一个文件</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        img_name = self.img_path[idx]</span><br><span class="line">        img_item_path = os.path.join(self.root_dir,self.label_dir,img_name)</span><br><span class="line">        <span class="comment"># 根据下标获取每一个元素</span></span><br><span class="line">        label = self.label_dir</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 数据集长度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br></pre></td></tr></table></figure>



<h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><p>就是TensorFlow的tensorboard</p>
<ul>
<li><p>如果不清除掉原来生成的文件，那么它会在原有的基础上继续绘制</p>
</li>
<li><p>运行命令</p>
<p><code>tensorboard --logdir=&#123;xxx&#125; --port=xxx</code></p>
</li>
<li><p>例子(报错需要 <code>pip install tensorboard</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tag:标题 scalar_value:y轴 global_step:x轴</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(tag=<span class="string">&quot;y=2x&quot;</span>,scalar_value=<span class="number">2</span>*i,global_step=i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h3><ul>
<li><p>将变量转为tensor向量、裁剪、resize等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;./pic/Snipaste_2021-05-30_15-08-36.png&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">tensor_img = tensor_trans(img)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize</span></span><br><span class="line"><span class="built_in">print</span>(tensor_img[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">tran_norm = transforms.Normalize([<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>],[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]) <span class="comment">#此处是三通道，因此mean和std都需要是三维向量</span></span><br><span class="line">img_norm = tran_norm(tensor_img)</span><br><span class="line"><span class="built_in">print</span>(img_norm[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize</span></span><br><span class="line"><span class="built_in">print</span>(img.size)</span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">img_resize = trans_resize(img)</span><br><span class="line"><span class="built_in">print</span>(img_resize.size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RandomCrop 随机裁剪</span></span><br><span class="line">trans_random = transforms.RandomCrop(<span class="number">256</span>)</span><br><span class="line">trans_compose = transforms.Compose([trans_random,tensor_trans])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_crop = trans_compose(img)</span><br><span class="line">    <span class="built_in">print</span>(img_crop.size)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>主要的作用是将变量包装，包装的信息包含了机器学习所需要的信息，比如反向传播需要的信息，梯度，等等</p>
<p><img src="https://raw.githubusercontent.com/CooperXJ/ImageBed/master/img/20210530151529.png" alt="image-20210530151518448"></p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><ul>
<li><p>DataSet</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compose 的作用就是将多个操作紧凑到一起 比如说转为tensor、裁剪等等</span></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment"># root 存放目录 train：是否为训练集还是测试集 download：本地没有的话是否去下载 transform:直接将数据集转为tensor</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=dataset_transform)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=dataset_transform)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(test_set.classes)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(test_set))</span><br><span class="line">img,target = test_set[<span class="number">0</span>] <span class="comment"># 每一个元素都由两个元素组成：图片，以及对应的图片类型</span></span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target])</span><br><span class="line"><span class="comment"># img.show()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;p10&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">10</span>):</span><br><span class="line">    img,target = test_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&quot;test_set&quot;</span>,img,i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li><p>DataLoader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=dataset_transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size 每次抓取数据的个数  shuffle：是否打乱 num_workers：多少个子线程去做load data（在windows下非0数会报错，0代表当前主线程） drop_last:是否丢弃最后剩余的数据  例如batch_size=3 总共有100个，为True则丢弃最后一个元素</span></span><br><span class="line">test_loader = DataLoader(dataset=test_set,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">img,target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    setp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        <span class="comment"># 此处的imgs: torch.Size([64, 3, 32, 32])  64位个数，后三个为图片RGB ， H ,W</span></span><br><span class="line">        <span class="comment"># 此处的targets: 为64个对应的target</span></span><br><span class="line">        imgs,targets = data</span><br><span class="line">        <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">        <span class="built_in">print</span>(targets)</span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch),imgs,setp)</span><br><span class="line">        setp+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h3><h4 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h4><p>以卷积为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为conv2d输入的参数input与kernel都需要是 input tensor of shape (minibatch,in_channels,iW)这种形式因此需要对其进行转换reshape一下</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">kernel = torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"><span class="built_in">print</span>(kernel.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># stride 每次移动的部长</span></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># padding 最外层添加一层 值都是0</span></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>



<h4 id="torch-nn-Model"><a href="#torch-nn-Model" class="headerlink" title="torch.nn.Model"></a>torch.nn.Model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line">test_loader = DataLoader(dataset=test_set,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Test, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test = Test()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./test&quot;</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    output = test(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#这里会变成64*2 因为是有6个channel，而原始的只有3个channel reshape的目的是为了将其重新转变为三通道的图片</span></span><br><span class="line">    output = torch.reshape(output,(-<span class="number">1</span>,<span class="number">3</span>,<span class="number">30</span>,<span class="number">30</span>))</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line"></span><br><span class="line">    step = step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>



<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,-<span class="number">0.5</span>],</span><br><span class="line">                      [-<span class="number">1</span>,<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,[-<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Test, self).__init__()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        output = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test = Test()</span><br><span class="line">output = test(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>



<h4 id="linear"><a href="#linear" class="headerlink" title="linear"></a>linear</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line">test_loader = DataLoader(dataset=test_set,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Test, self).__init__()</span><br><span class="line">        self.linear1 = Linear(<span class="number">196608</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span></span><br><span class="line">        output = self.linear1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">test = Test()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接将多维转为一维</span></span><br><span class="line">    output = torch.flatten(imgs)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">    output = test(output)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>



<h4 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Test, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x =self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test = Test()</span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构成测试数据，都是1</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">output = test(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs_seq&quot;</span>)</span><br><span class="line">writer.add_graph(test,<span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>



<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和</span></span><br><span class="line">loss = L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># mean 默认是mean</span></span><br><span class="line">loss = L1Loss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵</span></span><br><span class="line">x = torch.tensor([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x,(<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure>



<h4 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(test_set,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Test, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x =self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test = Test()</span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    outputs = test(imgs)</span><br><span class="line">    result_loss = loss(outputs,targets)</span><br><span class="line">    <span class="comment"># 这里千万要注意到此处的backward只能在result_loss下执行</span></span><br><span class="line">    result_loss.backward()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ok&quot;</span>)</span><br></pre></td></tr></table></figure>



<h4 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(test_set,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Test, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x =self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test = Test()</span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">optim = torch.optim.SGD(test.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        outputs = test(imgs)</span><br><span class="line">        result_loss = loss(outputs,targets)</span><br><span class="line">        <span class="comment"># 这里注意一定要全部置为0  因为上一轮对下一轮的训练没有用</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        <span class="comment"># 这里千万要注意到此处的backward只能在result_loss下执行</span></span><br><span class="line">        result_loss.backward()</span><br><span class="line">        <span class="comment"># 训练</span></span><br><span class="line">        optim.step()</span><br><span class="line"></span><br><span class="line">        running_loss+=result_loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>对学习率分阶段修改</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分步骤训练 就是不同训练次数选择不同的学习率（越来院小）</span></span><br><span class="line"><span class="comment"># 下面表示每训练五次学习率*0.1</span></span><br><span class="line">scheduler = StepLR(optim,step_size=<span class="number">5</span>,gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        outputs = test(imgs)</span><br><span class="line">        result_loss = loss(outputs,targets)</span><br><span class="line">        <span class="comment"># 这里注意一定要全部置为0  因为上一轮对下一轮的训练没有用</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        <span class="comment"># 这里千万要注意到此处的backward只能在result_loss下执行</span></span><br><span class="line">        result_loss.backward()</span><br><span class="line">        <span class="comment"># 训练</span></span><br><span class="line">        <span class="comment"># optim.step()</span></span><br><span class="line">        scheduler.step()</span><br><span class="line">        running_loss+=result_loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="加载官网训练好的模型以及在此基础上进行修改"><a href="#加载官网训练好的模型以及在此基础上进行修改" class="headerlink" title="加载官网训练好的模型以及在此基础上进行修改"></a>加载官网训练好的模型以及在此基础上进行修改</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为true的话会将整个模型下载下来，为false的话会在网络中加载该模型,但是是没有训练好的模型</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以数组的方式将第六层改变为nn.Linear(4096,10)</span></span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在classifier中添加一层</span></span><br><span class="line">vgg16_false.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接添加一层</span></span><br><span class="line">vgg16_false.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure>



<h4 id="模型save和load"><a href="#模型save和load" class="headerlink" title="模型save和load"></a>模型save和load</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1 方式1的缺陷在于在导入的时候需要模型的定义信息，就是class Mymodel(nn.Moudel)这个类需要加载进来，否则会报错</span></span><br><span class="line"><span class="comment"># save</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16,<span class="string">&quot;./model/vgg16_method1.pth&quot;</span>)</span><br><span class="line">model = torch.load(<span class="string">&quot;./model/vgg16_method1.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2  以字典的形式加载模型，将参数赋予到模型上</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&quot;./model/vgg16_method2.pth&quot;</span>)</span><br><span class="line">vgg = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg.load_state_dict(torch.load(<span class="string">&quot;./model/vgg16_method2.pth&quot;</span>))</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/35/">35</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cooper</p>
  <div class="site-description" itemprop="description">记录一些学习笔记、小说、随笔</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">341</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">75</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/CooperXJ" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;CooperXJ" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1789023580@qq.com" title="E-Mail → mailto:1789023580@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cooper</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">648k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">9:49</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'rOzi1T6uncSYNdx0tp1owOzV-gzGzoHsz',
      appKey     : 'Fk3s6CQBK042e1QltClHnTBP',
      placeholder: "留下邮箱，有回复时你将收到提醒，邮箱不会被公开",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
